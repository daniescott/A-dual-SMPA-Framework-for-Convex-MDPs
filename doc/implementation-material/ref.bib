@article{zahavy2021reward,
  title={Reward is enough for convex mdps},
  author={Zahavy, Tom and O'Donoghue, Brendan and Desjardins, Guillaume and Singh, Satinder},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={25746--25759},
  year={2021}
}

@inproceedings{asad2024fast,
title={Fast Convergence of Softmax Policy Mirror Ascent for Bandits \& Tabular {MDP}s},
author={Reza Asad and Reza Babanezhad Harikandeh and Issam H. Laradji and Nicolas Le Roux and Sharan Vaswani},
booktitle={OPT 2024: Optimization for Machine Learning},
year={2024},
url={https://openreview.net/forum?id=f5OjNMXIik}
}

@inproceedings{barakat2024global,
  title={On the Global Optimality of Policy Gradient Methods in General Utility Reinforcement Learning},
  author={Barakat, Anas and Chakraborty, Souradip and Yu, Pengcheng and Tokekar, Pratap and Bedi, Amrit Singh},
  booktitle={Advances in Neural Information Processing Systems},
  year={2024}
}

@article{DBLP:journals/corr/abs-2007-02151,
  author       = {Junyu Zhang and
                  Alec Koppel and
                  Amrit Singh Bedi and
                  Csaba Szepesv{\'{a}}ri and
                  Mengdi Wang},
  title        = {Variational Policy Gradient Method for Reinforcement Learning with
                  General Utilities},
  journal      = {CoRR},
  volume       = {abs/2007.02151},
  year         = {2020},
  url          = {https://arxiv.org/abs/2007.02151},
  eprinttype    = {arXiv},
  eprint       = {2007.02151},
  timestamp    = {Mon, 29 Sep 2025 09:04:14 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2007-02151.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2005-09814,
  author       = {Manan Tomar and
                  Lior Shani and
                  Yonathan Efroni and
                  Mohammad Ghavamzadeh},
  title        = {Mirror Descent Policy Optimization},
  journal      = {CoRR},
  volume       = {abs/2005.09814},
  year         = {2020},
  url          = {https://arxiv.org/abs/2005.09814},
  eprinttype    = {arXiv},
  eprint       = {2005.09814},
  timestamp    = {Fri, 22 May 2020 16:21:28 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2005-09814.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/SchulmanLMJA15,
  author       = {John Schulman and
                  Sergey Levine and
                  Philipp Moritz and
                  Michael I. Jordan and
                  Pieter Abbeel},
  title        = {Trust Region Policy Optimization},
  journal      = {CoRR},
  volume       = {abs/1502.05477},
  year         = {2015},
  url          = {http://arxiv.org/abs/1502.05477},
  eprinttype    = {arXiv},
  eprint       = {1502.05477},
  timestamp    = {Mon, 13 Aug 2018 16:48:08 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/SchulmanLMJA15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/SchulmanWDRK17,
  author       = {John Schulman and
                  Filip Wolski and
                  Prafulla Dhariwal and
                  Alec Radford and
                  Oleg Klimov},
  title        = {Proximal Policy Optimization Algorithms},
  journal      = {CoRR},
  volume       = {abs/1707.06347},
  year         = {2017},
  url          = {http://arxiv.org/abs/1707.06347},
  eprinttype    = {arXiv},
  eprint       = {1707.06347},
  timestamp    = {Mon, 13 Aug 2018 16:47:34 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/SchulmanWDRK17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1906-09323,
  author       = {Sobhan Miryoosefi and
                  Kiant{\'{e}} Brantley and
                  Hal Daum{\'{e}} III and
                  Miroslav Dud{\'{\i}}k and
                  Robert E. Schapire},
  title        = {Reinforcement Learning with Convex Constraints},
  journal      = {CoRR},
  volume       = {abs/1906.09323},
  year         = {2019},
  url          = {http://arxiv.org/abs/1906.09323},
  eprinttype    = {arXiv},
  eprint       = {1906.09323},
  timestamp    = {Thu, 27 Jun 2019 18:54:51 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1906-09323.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1812-02690,
  author       = {Elad Hazan and
                  Sham M. Kakade and
                  Karan Singh and
                  Abby Van Soest},
  title        = {Provably Efficient Maximum Entropy Exploration},
  journal      = {CoRR},
  volume       = {abs/1812.02690},
  year         = {2018},
  url          = {http://arxiv.org/abs/1812.02690},
  eprinttype    = {arXiv},
  eprint       = {1812.02690},
  timestamp    = {Tue, 01 Jan 2019 15:01:25 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1812-02690.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{ying2024policybasedprimaldualmethodsconcave,
      title={Policy-based Primal-Dual Methods for Concave CMDP with Variance Reduction}, 
      author={Donghao Ying and Mengzi Amy Guo and Hyunin Lee and Yuhao Ding and Javad Lavaei and Zuo-Jun Max Shen},
      year={2024},
      eprint={2205.10715},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2205.10715}, 
}


@InProceedings{pmlr-v70-achiam17a,
  title = 	 {Constrained Policy Optimization},
  author =       {Joshua Achiam and David Held and Aviv Tamar and Pieter Abbeel},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {22--31},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/achiam17a/achiam17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/achiam17a.html},
  abstract = 	 {For many applications of reinforcement learning it can be more convenient to specify both a reward function and constraints, rather than trying to design behavior through the reward function. For example, systems that physically interact with or around humans should satisfy safety constraints. Recent advances in policy search algorithms (Mnih et al., 2016, Schulman et al., 2015, Lillicrap et al., 2016, Levine et al., 2016) have enabled new capabilities in high-dimensional control, but do not consider the constrained setting. We propose Constrained Policy Optimization (CPO), the first general-purpose policy search algorithm for constrained reinforcement learning with guarantees for near-constraint satisfaction at each iteration. Our method allows us to train neural network policies for high-dimensional control while making guarantees about policy behavior all throughout training. Our guarantees are based on a new theoretical result, which is of independent interest: we prove a bound relating the expected returns of two policies to an average divergence between them. We demonstrate the effectiveness of our approach on simulated robot locomotion tasks where the agent must satisfy constraints motivated by safety.}
}


@book{sutton2018reinforcement,
  title        = {Reinforcement Learning: An Introduction},
  author       = {Sutton, Richard S. and Barto, Andrew G.},
  edition      = {2nd},
  year         = {2018},
  publisher    = {The MIT Press},
  address      = {Cambridge, MA; London, England},
  note         = {Electronic version (PDF) available online},
}


@article{DBLP:journals/corr/abs-1801-01290,
  author       = {Tuomas Haarnoja and
                  Aurick Zhou and
                  Pieter Abbeel and
                  Sergey Levine},
  title        = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning
                  with a Stochastic Actor},
  journal      = {CoRR},
  volume       = {abs/1801.01290},
  year         = {2018},
  url          = {http://arxiv.org/abs/1801.01290},
  eprinttype    = {arXiv},
  eprint       = {1801.01290},
  timestamp    = {Mon, 13 Aug 2018 16:48:10 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1801-01290.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1906-00331,
  author       = {Tianyi Lin and
                  Chi Jin and
                  Michael I. Jordan},
  title        = {On Gradient Descent Ascent for Nonconvex-Concave Minimax Problems},
  journal      = {CoRR},
  volume       = {abs/1906.00331},
  year         = {2019},
  url          = {http://arxiv.org/abs/1906.00331},
  eprinttype    = {arXiv},
  eprint       = {1906.00331},
  timestamp    = {Wed, 07 Dec 2022 22:58:56 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1906-00331.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{1370862715914709505,
author="Nesterov, Y.",
title="A method for solving the convex programming problem with convergence rate o(1/k2)",
journal="Dokl Akad Nauk SSSR",
year="1983",
volume="269",
pages="543",
URL="https://cir.nii.ac.jp/crid/1370862715914709505"
}

@misc{kingma2017adammethodstochasticoptimization,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1412.6980}, 
}

@misc{lascu2025ppofisherraogeometry,
      title={PPO in the Fisher-Rao geometry}, 
      author={Razvan-Andrei Lascu and David Šiška and Łukasz Szpruch},
      year={2025},
      eprint={2506.03757},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2506.03757}, 
}

@misc{shani2019adaptivetrustregionpolicy,
      title={Adaptive Trust Region Policy Optimization: Global Convergence and Faster Rates for Regularized MDPs}, 
      author={Lior Shani and Yonathan Efroni and Shie Mannor},
      year={2019},
      eprint={1909.02769},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1909.02769}, 
}